model:
  bert_name: "bert-base-uncased"
  gpt_name: "gpt2"

data:
  train_sample_size: 2000
  val_sample_size: 500
  max_length: 256

training:
  batch_size: 8
  learning_rate: 5e-5
  epochs: 2

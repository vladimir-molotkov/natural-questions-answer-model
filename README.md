Training GPT model to answer natural questions.
# Постановка задачи
В данном репозитории создана модель, отвечающая на вопросы по естественнонаучным дисциплинам по информации из Википедии.
Производится сравнение BERT и GPT2 на тестовой выборке, затем модель GPT2 дообучается для повышения качества.

### Формат входных и выходных данных
Данные представлены в текстовом формате (файлы формата parquet) в виде пар "вопрос-ответ". В датасете представлены короткие и длинные ответы, в данном репозитории используются только короткие ответы.

### Метрика
В качестве метрики качества модели выбрана F1-мера по совпадению токенов. В данном случае требуется получить метрики выше бейзлайна BERT.

### Валидация
Датасет представлен в виде обучающей и валидационной выборки:
- Обучающая выборка состоит 307 тысяч примеров.
- Валидационная выборка фиксирована и состоит из 7 тысяч примеров.

### Данные
В данной работе используется датасет [Natural Questions](https://huggingface.co/datasets/google-research-datasets/natural_questions) от Google Research.
Датасет содержит выборку реальных вопросов из поисковой системы Google по естественным наукам с соответствующими ответами из топ-5 страниц Википедии и ссылками на источники. Ответы разделены на короткие и длинные. Для обучения модели отобраны пары "вопрос-ответ" только с короткими ответами.

### Бейзлайн
В качестве бейзлайн модели используется BERT.

### Основная модель
В качестве основной модели для дообучения предполагается использовать GPT-2.

### Внедрение
Модель может быть использована в качестве простого чат-бота для ответа на вопросы по естестенным науками. Кроме того, модель не требует значительных ресурсов для запуска.
